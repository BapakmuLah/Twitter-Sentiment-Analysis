{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.corpus import twitter_samples\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     c:\\Tensorflow\\NLP\\NLP Twitter...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DOWNLOAD DATASET\n",
    "\n",
    "nltk.download('twitter_samples', download_dir= os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'README.md', 'README.txt', 'tweets.20150430-223406.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the contents of the directory\n",
    "print(os.listdir(r'C:\\Tensorflow\\NLP\\NLP Twitter\\corpora\\twitter_samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Tweets has 5000 samples\n",
      "Negative Tweets has 5000 samples\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATASET \n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# MOVE NLTK TO THAT PATH\n",
    "nltk.data.path.append(r'C:\\Tensorflow\\NLP\\NLP Twitter')\n",
    "\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print(f'Positive Tweets has {len(all_positive_tweets)} samples')\n",
    "print(f'Negative Tweets has {len(all_negative_tweets)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display 10 Positive Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1 : #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "Tweet 2 : @Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "Tweet 3 : @DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "Tweet 4 : @97sides CONGRATS :)\n",
      "Tweet 5 : yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
      "Tweet 6 : @BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
      "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n",
      "Tweet 7 : We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\n",
      "Tweet 8 : @Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.\n",
      "Tweet 9 : Jgh , but we have to go to Bayan :D bye\n",
      "Tweet 10 : As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\n",
      "\n",
      "Well… as the name implies :p.\n"
     ]
    }
   ],
   "source": [
    "# DISPLAY 5 SAMPLES FROM POSITIVE TWEETS\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f'Tweet {i+1} : {all_positive_tweets[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1 : hopeless for tmr :(\n",
      "Tweet 2 : Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
      "Tweet 3 : @Hegelbon That heart sliding into the waste basket. :(\n",
      "Tweet 4 : “@ketchBurning: I hate Japanese call him \"bani\" :( :(”\n",
      "\n",
      "Me too\n",
      "Tweet 5 : Dang starting next week I have \"work\" :(\n",
      "Tweet 6 : oh god, my babies' faces :( https://t.co/9fcwGvaki0\n",
      "Tweet 7 : @RileyMcDonough make me smile :((\n",
      "Tweet 8 : @f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln\n",
      "Tweet 9 : why?:(\"@tahuodyy: sialan:( https://t.co/Hv1i0xcrL2\"\n",
      "Tweet 10 : Athabasca glacier was there in #1948 :-( #athabasca #glacier #jasper #jaspernationalpark #alberta #explorealberta #… http://t.co/dZZdqmf7Cz\n"
     ]
    }
   ],
   "source": [
    "# DISPLAY 5 SAMPLES FROM NEGATIVE TWEETS\n",
    "\n",
    "i = 0\n",
    "\n",
    "for tweet in all_negative_tweets:\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "    print(f'Tweet {i+1} : {tweet}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Raw Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000,\n",
       " 2000,\n",
       " array(['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
       "        '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!'],\n",
       "       dtype='<U152'),\n",
       " array(['Bro:U wan cut hair anot,ur hair long Liao bo\\nMe:since ord liao,take it easy lor treat as save $ leave it longer :)\\nBro:LOL Sibei xialan',\n",
       "        \"@heyclaireee is back! thnx God!!! i'm so happy :)\"], dtype='<U146'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPLIT DATA INTO TRAIN AND TEST\n",
    "\n",
    "train_data = np.concatenate((all_positive_tweets[:4000], all_negative_tweets[:4000]), axis=0)\n",
    "test_data  = np.concatenate((all_positive_tweets[4000:5000], all_negative_tweets[4000:5000]), axis=0)\n",
    "\n",
    "len(train_data) , len(test_data) , train_data[:2] , test_data[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Label for every Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000,\n",
       " 2000,\n",
       " array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATE LABEL . IF THEY POSITIVE TWEET, GIVE THEM 1  .   AND IF THEY NEGATIVE TWEET , GIVE THEM 0\n",
    "\n",
    "train_positive = all_positive_tweets[:4000]\n",
    "train_negative = all_negative_tweets[:4000]\n",
    "\n",
    "test_positive = all_positive_tweets[4000:5000]\n",
    "test_negative = all_negative_tweets[4000:5000]\n",
    "\n",
    "#            FILL VALUE 1        shape=(4000,1)          FILL VALUE 0  shape=(4000,1)\n",
    "train_label = np.append(np.ones( shape=( len(train_positive),1) ), np.zeros( shape=( len(train_negative),1) ), axis=0)\n",
    "\n",
    "#           FILL VALUE 1         shape=(1000,1)          FILL VALUE 0  shape=(1000,1)\n",
    "test_label  = np.append(np.ones( shape=( len(test_positive),1) ) , np.zeros( shape=( len(test_negative),1) ), axis=0)\n",
    "\n",
    "\n",
    "# DEBUGGING\n",
    "len(train_label) , len(test_label) , train_label , test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>Amelia didnt stalk my twitter :(</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>oh, i missed the broadcast. : (</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>i really can't stream on melon i feel useless :-(</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>I need to stop looking at old soccer pictures :(</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>Got an interview for the job that I want but t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweets  Label\n",
       "0     #FollowFriday @France_Inte @PKuchly57 @Milipol...    1.0\n",
       "1     @Lamb2ja Hey James! How odd :/ Please call our...    1.0\n",
       "2     @DespiteOfficial we had a listen last night :)...    1.0\n",
       "3                                  @97sides CONGRATS :)    1.0\n",
       "4     yeaaaah yippppy!!!  my accnt verified rqst has...    1.0\n",
       "...                                                 ...    ...\n",
       "7995                   Amelia didnt stalk my twitter :(    0.0\n",
       "7996                    oh, i missed the broadcast. : (    0.0\n",
       "7997  i really can't stream on melon i feel useless :-(    0.0\n",
       "7998   I need to stop looking at old soccer pictures :(    0.0\n",
       "7999  Got an interview for the job that I want but t...    0.0\n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VISUALIZE DATASET TWEETS USING DATAFRAME \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Tweets': train_data,\n",
    "    'Label' : train_label.reshape(-1)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING TWEETS\n",
    "\n",
    "import re  # USING REGEX TO CLEANING TEXT\n",
    "\n",
    "def TextCleaning(tweet):\n",
    "\n",
    "    # REMOVE HYPERLINK\n",
    "    cleaning_tweets = re.sub(pattern=r'https?://[^\\n\\r\\s]+', repl= '', string= tweet)\n",
    "\n",
    "    # REMOVE USENAME\n",
    "    cleaning_tweets = re.sub(pattern=r'@\\w+', repl='', string= cleaning_tweets)\n",
    "\n",
    "    # REMOVE HASHTAGS\n",
    "    cleaning_tweets = re.sub(pattern=r'#', repl='', string= cleaning_tweets)\n",
    "\n",
    "    return cleaning_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Clean :\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "@97sides CONGRATS :)\n",
      "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
      "\n",
      "After Clean :\n",
      "FollowFriday    for being top engaged members in my community this week :)\n",
      " Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      " we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      " CONGRATS :)\n",
      "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n"
     ]
    }
   ],
   "source": [
    "# DISPLAY TWEETS AFTER CLEANED\n",
    "\n",
    "# BEFORE CLEANING\n",
    "print('Before Clean :')\n",
    "for tweet in train_data[:5]:\n",
    "    print(tweet)\n",
    "\n",
    "i = 0\n",
    "cleaned_tweets = []\n",
    "\n",
    "print('\\nAfter Clean :')\n",
    "for tweet in train_data:\n",
    "    clean_tweet = TextCleaning(tweet)\n",
    "\n",
    "    # JUST PRINT THE FIRST 5 TWEET\n",
    "    if i < 5:\n",
    "        print(clean_tweet)\n",
    "    i += 1\n",
    "    cleaned_tweets.append(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM TWEET INTO EACH TOKEN\n",
    "\n",
    "def Tokenization(tweet):\n",
    "\n",
    "    # DECLARE TOKENIZER\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "    # TRANSFORM TO EVERY TWEETS/STRINGS\n",
    "    tweet_token= tokenizer.tokenize(tweet)\n",
    "\n",
    "    return tweet_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet After Tokenization :\n",
      "['followfriday', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "['hey', 'james', '!', 'how', 'odd', ':/', 'please', 'call', 'our', 'contact', 'centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'many', 'thanks', '!']\n",
      "['we', 'had', 'a', 'listen', 'last', 'night', ':)', 'as', 'you', 'bleed', 'is', 'an', 'amazing', 'track', '.', 'when', 'are', 'you', 'in', 'scotland', '?', '!']\n",
      "['congrats', ':)']\n",
      "['yeaaah', 'yipppy', '!', '!', '!', 'my', 'accnt', 'verified', 'rqst', 'has', 'succeed', 'got', 'a', 'blue', 'tick', 'mark', 'on', 'my', 'fb', 'profile', ':)', 'in', '15', 'days']\n"
     ]
    }
   ],
   "source": [
    "# DISPLAY TWEET AFTER TOKENIZATION\n",
    "\n",
    "print('Tweet After Tokenization :')\n",
    "\n",
    "i = 0\n",
    "tweet_tokens = []\n",
    "\n",
    "for tweet in cleaned_tweets:\n",
    "    tweet_token = Tokenization(tweet)\n",
    "\n",
    "    # JUST PRINT THE FIRST 5 TWEET\n",
    "    if i < 5:\n",
    "        print(tweet_token)\n",
    "\n",
    "    i += 1\n",
    "    tweet_tokens.append(tweet_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size stopwords : 179\n",
      "Stopwords : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n",
      "Punctuation : !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aliff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# GET STOPWORDS AND PUNCTUATION\n",
    "\n",
    "# DOWNLOAD STOPWORDS\n",
    "nltk.download(info_or_id='stopwords')\n",
    "\n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "print(f'size stopwords : {len(stopwords_english)}')\n",
    "print(f'Stopwords : {stopwords_english[:20]}')  # DISPLAY 20 STOPWORDS\n",
    "\n",
    "print(f'Punctuation : {string.punctuation}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE STOPWORDS AND PUNCTUATION\n",
    "\n",
    "def remove_StopWords(tweet):\n",
    "\n",
    "    cleaned_token = []\n",
    "\n",
    "    # ITERATE EVERY TOKEN IN A TWEET         \n",
    "    for word in tweet:   \n",
    "\n",
    "        # IF TOKEN DOESNT CONTAIN STOPWORDS AND PUNCTUATION\n",
    "        if (word not in stopwords_english and word not in string.punctuation):\n",
    "            cleaned_token.append(word) # PUSH THAT TOKEN\n",
    "                \n",
    "\n",
    "    return cleaned_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Removed Stopwords and Punctuation :\n",
      "['followfriday', 'top', 'engaged', 'members', 'community', 'week', ':)']\n",
      "['hey', 'james', 'odd', ':/', 'please', 'call', 'contact', 'centre', '02392441234', 'able', 'assist', ':)', 'many', 'thanks']\n",
      "['listen', 'last', 'night', ':)', 'bleed', 'amazing', 'track', 'scotland']\n",
      "['congrats', ':)']\n",
      "['yeaaah', 'yipppy', 'accnt', 'verified', 'rqst', 'succeed', 'got', 'blue', 'tick', 'mark', 'fb', 'profile', ':)', '15', 'days']\n"
     ]
    }
   ],
   "source": [
    "# DISPLAY TWEET AFTER REMOVED STOP WORDS\n",
    "\n",
    "i = 0\n",
    "cleaned_tweets = []\n",
    "\n",
    "print('After Removed Stopwords and Punctuation :')\n",
    "for tweet in tweet_tokens:\n",
    "    tweet_clean = remove_StopWords(tweet)\n",
    "\n",
    "    # JUST PRINT THE FIRST 5 TWEET\n",
    "    if i < 5:\n",
    "        print(tweet_clean)\n",
    "\n",
    "    i += 1\n",
    "    cleaned_tweets.append(tweet_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEMMING USING PORTER STEMMING ALGORITHM\n",
    "\n",
    "\n",
    "def stemming(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    tweet_stemmed = []  # TO STORE STEMMED TWEET\n",
    "    \n",
    "        # ITERATE EACH WORD/TOKEN IN TWEET\n",
    "    for word in tweet:\n",
    "        stem_word = stemmer.stem(word)  # STEMMING\n",
    "        tweet_stemmed.append(stem_word)  # PUSH STEMMED TOKEN\n",
    "\n",
    "    return tweet_stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet After Stemming : \n",
      "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
      "['hey', 'jame', 'odd', ':/', 'pleas', 'call', 'contact', 'centr', '02392441234', 'abl', 'assist', ':)', 'mani', 'thank']\n",
      "['listen', 'last', 'night', ':)', 'bleed', 'amaz', 'track', 'scotland']\n",
      "['congrat', ':)']\n",
      "['yeaaah', 'yipppi', 'accnt', 'verifi', 'rqst', 'succeed', 'got', 'blue', 'tick', 'mark', 'fb', 'profil', ':)', '15', 'day']\n"
     ]
    }
   ],
   "source": [
    "# DISPLAY TWEET/TOKEN AFTER STEMMING\n",
    "\n",
    "i = 0\n",
    "tweet_stemmed = []\n",
    "\n",
    "print('Tweet After Stemming : ')\n",
    "\n",
    "for tweet in cleaned_tweets:\n",
    "    stem = stemming(tweet)\n",
    "\n",
    "    # JUST PRINT THE FIRST 5 TWEET\n",
    "    if i < 5:\n",
    "        print(stem)\n",
    "\n",
    "    i += 1\n",
    "    tweet_stemmed.append(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Each Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliff\\AppData\\Local\\Temp\\ipykernel_16920\\876181935.py:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pair = (word, int(label))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11420,\n",
       " {('followfriday', 1): 23,\n",
       "  ('top', 1): 30,\n",
       "  ('engag', 1): 7,\n",
       "  ('member', 1): 14,\n",
       "  ('commun', 1): 27,\n",
       "  ('week', 1): 72,\n",
       "  (':)', 1): 2960,\n",
       "  ('hey', 1): 60,\n",
       "  ('jame', 1): 7,\n",
       "  ('odd', 1): 2,\n",
       "  (':/', 1): 5,\n",
       "  ('pleas', 1): 81,\n",
       "  ('call', 1): 27,\n",
       "  ('contact', 1): 4,\n",
       "  ('centr', 1): 1,\n",
       "  ('02392441234', 1): 1,\n",
       "  ('abl', 1): 6,\n",
       "  ('assist', 1): 1,\n",
       "  ('mani', 1): 28,\n",
       "  ('thank', 1): 522,\n",
       "  ('listen', 1): 15,\n",
       "  ('last', 1): 39,\n",
       "  ('night', 1): 55,\n",
       "  ('bleed', 1): 2,\n",
       "  ('amaz', 1): 41,\n",
       "  ('track', 1): 5,\n",
       "  ('scotland', 1): 2,\n",
       "  ('congrat', 1): 15,\n",
       "  ('yeaaah', 1): 1,\n",
       "  ('yipppi', 1): 1,\n",
       "  ('accnt', 1): 2,\n",
       "  ('verifi', 1): 2,\n",
       "  ('rqst', 1): 1,\n",
       "  ('succeed', 1): 1,\n",
       "  ('got', 1): 57,\n",
       "  ('blue', 1): 8,\n",
       "  ('tick', 1): 1,\n",
       "  ('mark', 1): 1,\n",
       "  ('fb', 1): 4,\n",
       "  ('profil', 1): 2,\n",
       "  ('15', 1): 4,\n",
       "  ('day', 1): 187,\n",
       "  ('one', 1): 92,\n",
       "  ('irresist', 1): 2,\n",
       "  ('flipkartfashionfriday', 1): 16,\n",
       "  ('like', 1): 187,\n",
       "  ('keep', 1): 55,\n",
       "  ('love', 1): 336,\n",
       "  ('custom', 1): 4,\n",
       "  ('wait', 1): 55,\n",
       "  ('long', 1): 27,\n",
       "  ('hope', 1): 115,\n",
       "  ('enjoy', 1): 61,\n",
       "  ('happi', 1): 162,\n",
       "  ('friday', 1): 91,\n",
       "  ('lwwf', 1): 1,\n",
       "  ('second', 1): 8,\n",
       "  ('thought', 1): 21,\n",
       "  ('’', 1): 17,\n",
       "  ('enough', 1): 16,\n",
       "  ('time', 1): 101,\n",
       "  ('dd', 1): 1,\n",
       "  ('new', 1): 114,\n",
       "  ('short', 1): 6,\n",
       "  ('enter', 1): 9,\n",
       "  ('system', 1): 2,\n",
       "  ('sheep', 1): 1,\n",
       "  ('must', 1): 14,\n",
       "  ('buy', 1): 10,\n",
       "  ('jgh', 1): 4,\n",
       "  ('go', 1): 123,\n",
       "  ('bayan', 1): 1,\n",
       "  (':d', 1): 523,\n",
       "  ('bye', 1): 5,\n",
       "  ('act', 1): 6,\n",
       "  ('mischiev', 1): 1,\n",
       "  ('etl', 1): 1,\n",
       "  ('layer', 1): 1,\n",
       "  ('in-hous', 1): 1,\n",
       "  ('wareh', 1): 1,\n",
       "  ('app', 1): 12,\n",
       "  ('katamari', 1): 1,\n",
       "  ('well', 1): 66,\n",
       "  ('…', 1): 31,\n",
       "  ('name', 1): 12,\n",
       "  ('impli', 1): 1,\n",
       "  (':p', 1): 105,\n",
       "  ('influenc', 1): 16,\n",
       "  ('big', 1): 28,\n",
       "  ('...', 1): 228,\n",
       "  ('juici', 1): 3,\n",
       "  ('selfi', 1): 11,\n",
       "  ('follow', 1): 385,\n",
       "  ('u', 1): 204,\n",
       "  ('back', 1): 139,\n",
       "  ('perfect', 1): 17,\n",
       "  ('alreadi', 1): 19,\n",
       "  ('know', 1): 128,\n",
       "  (\"what'\", 1): 14,\n",
       "  ('great', 1): 135,\n",
       "  ('opportun', 1): 17,\n",
       "  ('junior', 1): 2,\n",
       "  ('triathlet', 1): 1,\n",
       "  ('age', 1): 2,\n",
       "  ('12', 1): 5,\n",
       "  ('13', 1): 5,\n",
       "  ('gatorad', 1): 1,\n",
       "  ('seri', 1): 4,\n",
       "  ('get', 1): 166,\n",
       "  ('entri', 1): 3,\n",
       "  ('lay', 1): 3,\n",
       "  ('greet', 1): 4,\n",
       "  ('card', 1): 6,\n",
       "  ('rang', 1): 2,\n",
       "  ('print', 1): 4,\n",
       "  ('today', 1): 91,\n",
       "  ('job', 1): 34,\n",
       "  (':-)', 1): 552,\n",
       "  (\"friend'\", 1): 3,\n",
       "  ('lunch', 1): 3,\n",
       "  ('yummm', 1): 1,\n",
       "  ('nostalgia', 1): 1,\n",
       "  ('tb', 1): 1,\n",
       "  ('ku', 1): 1,\n",
       "  ('id', 1): 8,\n",
       "  ('conflict', 1): 1,\n",
       "  ('help', 1): 40,\n",
       "  (\"here'\", 1): 20,\n",
       "  ('screenshot', 1): 2,\n",
       "  ('work', 1): 89,\n",
       "  ('hi', 1): 154,\n",
       "  ('liv', 1): 2,\n",
       "  ('hello', 1): 49,\n",
       "  ('need', 1): 62,\n",
       "  ('someth', 1): 25,\n",
       "  ('fm', 1): 2,\n",
       "  ('twitter', 1): 25,\n",
       "  ('—', 1): 22,\n",
       "  ('sure', 1): 38,\n",
       "  ('thing', 1): 48,\n",
       "  ('dm', 1): 34,\n",
       "  ('x', 1): 50,\n",
       "  (\"i'v\", 1): 25,\n",
       "  ('heard', 1): 9,\n",
       "  ('four', 1): 5,\n",
       "  ('season', 1): 5,\n",
       "  ('pretti', 1): 17,\n",
       "  ('dope', 1): 2,\n",
       "  ('penthous', 1): 1,\n",
       "  ('obv', 1): 1,\n",
       "  ('gobigorgohom', 1): 1,\n",
       "  ('fun', 1): 45,\n",
       "  (\"y'all\", 1): 4,\n",
       "  ('yeah', 1): 30,\n",
       "  ('suppos', 1): 6,\n",
       "  ('lol', 1): 48,\n",
       "  ('chat', 1): 9,\n",
       "  ('bit', 1): 16,\n",
       "  ('youth', 1): 14,\n",
       "  ('💅🏽', 1): 1,\n",
       "  ('💋', 1): 2,\n",
       "  ('seen', 1): 6,\n",
       "  ('year', 1): 33,\n",
       "  ('rest', 1): 9,\n",
       "  ('goe', 1): 4,\n",
       "  ('quickli', 1): 3,\n",
       "  ('bed', 1): 8,\n",
       "  ('music', 1): 15,\n",
       "  ('fix', 1): 6,\n",
       "  ('dream', 1): 17,\n",
       "  ('spiritu', 1): 1,\n",
       "  ('ritual', 1): 1,\n",
       "  ('festiv', 1): 7,\n",
       "  ('népal', 1): 1,\n",
       "  ('begin', 1): 4,\n",
       "  ('line-up', 1): 4,\n",
       "  ('left', 1): 10,\n",
       "  ('see', 1): 157,\n",
       "  ('sarah', 1): 4,\n",
       "  ('send', 1): 18,\n",
       "  ('us', 1): 96,\n",
       "  ('email', 1): 22,\n",
       "  ('bitsy.com', 1): 1,\n",
       "  (\"we'll\", 1): 12,\n",
       "  ('asap', 1): 5,\n",
       "  ('kik', 1): 16,\n",
       "  ('hatessuc', 1): 1,\n",
       "  ('32429', 1): 1,\n",
       "  ('kikm', 1): 1,\n",
       "  ('lgbt', 1): 2,\n",
       "  ('tinder', 1): 1,\n",
       "  ('nsfw', 1): 1,\n",
       "  ('akua', 1): 1,\n",
       "  ('cumshot', 1): 1,\n",
       "  ('come', 1): 63,\n",
       "  ('hous', 1): 5,\n",
       "  ('nsn_supplement', 1): 1,\n",
       "  ('effect', 1): 2,\n",
       "  ('press', 1): 1,\n",
       "  ('releas', 1): 11,\n",
       "  ('distribut', 1): 1,\n",
       "  ('result', 1): 2,\n",
       "  ('link', 1): 14,\n",
       "  ('remov', 1): 3,\n",
       "  ('pressreleas', 1): 1,\n",
       "  ('newsdistribut', 1): 1,\n",
       "  ('bam', 1): 44,\n",
       "  ('bestfriend', 1): 50,\n",
       "  ('lot', 1): 80,\n",
       "  ('warsaw', 1): 44,\n",
       "  ('<3', 1): 119,\n",
       "  ('x46', 1): 1,\n",
       "  ('everyon', 1): 45,\n",
       "  ('watch', 1): 32,\n",
       "  ('documentari', 1): 1,\n",
       "  ('earthl', 1): 1,\n",
       "  ('youtub', 1): 8,\n",
       "  ('support', 1): 25,\n",
       "  ('buuut', 1): 1,\n",
       "  ('oh', 1): 44,\n",
       "  ('look', 1): 111,\n",
       "  ('forward', 1): 20,\n",
       "  ('visit', 1): 25,\n",
       "  ('next', 1): 37,\n",
       "  ('letsgetmessi', 1): 1,\n",
       "  ('jo', 1): 1,\n",
       "  ('make', 1): 70,\n",
       "  ('feel', 1): 33,\n",
       "  ('better', 1): 40,\n",
       "  ('never', 1): 31,\n",
       "  ('anyon', 1): 7,\n",
       "  ('kpop', 1): 1,\n",
       "  ('flesh', 1): 1,\n",
       "  ('good', 1): 191,\n",
       "  ('girl', 1): 34,\n",
       "  ('best', 1): 49,\n",
       "  ('wish', 1): 29,\n",
       "  ('reason', 1): 10,\n",
       "  ('epic', 1): 1,\n",
       "  ('soundtrack', 1): 1,\n",
       "  ('shout', 1): 11,\n",
       "  ('ad', 1): 10,\n",
       "  ('video', 1): 30,\n",
       "  ('playlist', 1): 5,\n",
       "  ('im', 1): 41,\n",
       "  ('twitch', 1): 7,\n",
       "  ('leagu', 1): 6,\n",
       "  ('1', 1): 63,\n",
       "  ('4', 1): 23,\n",
       "  ('would', 1): 70,\n",
       "  ('dear', 1): 15,\n",
       "  ('jordan', 1): 1,\n",
       "  ('okay', 1): 31,\n",
       "  ('fake', 1): 1,\n",
       "  ('gameplay', 1): 1,\n",
       "  (';)', 1): 22,\n",
       "  ('haha', 1): 44,\n",
       "  ('kid', 1): 13,\n",
       "  ('stuff', 1): 11,\n",
       "  ('exactli', 1): 5,\n",
       "  ('product', 1): 11,\n",
       "  ('line', 1): 6,\n",
       "  ('etsi', 1): 1,\n",
       "  ('shop', 1): 12,\n",
       "  ('check', 1): 39,\n",
       "  ('boxroomcraft', 1): 1,\n",
       "  ('vacat', 1): 5,\n",
       "  ('recharg', 1): 1,\n",
       "  ('normal', 1): 5,\n",
       "  ('charger', 1): 2,\n",
       "  ('asleep', 1): 7,\n",
       "  ('talk', 1): 38,\n",
       "  ('sooo', 1): 6,\n",
       "  ('someon', 1): 29,\n",
       "  ('text', 1): 12,\n",
       "  ('ye', 1): 60,\n",
       "  ('bet', 1): 6,\n",
       "  (\"he'll\", 1): 2,\n",
       "  ('fit', 1): 2,\n",
       "  ('hear', 1): 24,\n",
       "  ('speech', 1): 1,\n",
       "  ('piti', 1): 2,\n",
       "  ('green', 1): 2,\n",
       "  ('garden', 1): 5,\n",
       "  ('midnight', 1): 1,\n",
       "  ('sun', 1): 6,\n",
       "  ('beauti', 1): 45,\n",
       "  ('canal', 1): 1,\n",
       "  ('dasvidaniya', 1): 1,\n",
       "  ('till', 1): 16,\n",
       "  ('scout', 1): 1,\n",
       "  ('sg', 1): 1,\n",
       "  ('futur', 1): 9,\n",
       "  ('wlan', 1): 1,\n",
       "  ('pro', 1): 4,\n",
       "  ('confer', 1): 1,\n",
       "  ('asia', 1): 1,\n",
       "  ('chang', 1): 20,\n",
       "  ('lollipop', 1): 1,\n",
       "  ('🍭', 1): 1,\n",
       "  ('nez', 1): 1,\n",
       "  ('agnezmo', 1): 1,\n",
       "  ('oley', 1): 1,\n",
       "  ('mama', 1): 1,\n",
       "  ('stand', 1): 6,\n",
       "  ('stronger', 1): 1,\n",
       "  ('god', 1): 14,\n",
       "  ('misti', 1): 1,\n",
       "  ('babi', 1): 17,\n",
       "  ('cute', 1): 22,\n",
       "  ('woohoo', 1): 3,\n",
       "  (\"can't\", 1): 31,\n",
       "  ('sign', 1): 9,\n",
       "  ('yet', 1): 12,\n",
       "  ('still', 1): 37,\n",
       "  ('think', 1): 54,\n",
       "  ('mka', 1): 5,\n",
       "  ('liam', 1): 5,\n",
       "  ('access', 1): 3,\n",
       "  ('welcom', 1): 54,\n",
       "  ('stat', 1): 51,\n",
       "  ('arriv', 1): 57,\n",
       "  ('unfollow', 1): 53,\n",
       "  ('via', 1): 73,\n",
       "  ('surpris', 1): 10,\n",
       "  ('figur', 1): 5,\n",
       "  ('happybirthdayemilybett', 1): 1,\n",
       "  ('sweet', 1): 16,\n",
       "  ('talent', 1): 4,\n",
       "  ('2', 1): 41,\n",
       "  ('plan', 1): 21,\n",
       "  ('drain', 1): 1,\n",
       "  ('gotta', 1): 4,\n",
       "  ('timezon', 1): 1,\n",
       "  ('parent', 1): 4,\n",
       "  ('proud', 1): 11,\n",
       "  ('least', 1): 14,\n",
       "  ('mayb', 1): 17,\n",
       "  ('sometim', 1): 11,\n",
       "  ('grade', 1): 4,\n",
       "  ('al', 1): 3,\n",
       "  ('grand', 1): 4,\n",
       "  ('manila_bro', 1): 1,\n",
       "  ('chosen', 1): 1,\n",
       "  ('let', 1): 70,\n",
       "  ('around', 1): 14,\n",
       "  ('..', 1): 100,\n",
       "  ('side', 1): 13,\n",
       "  ('world', 1): 23,\n",
       "  ('eh', 1): 2,\n",
       "  ('take', 1): 30,\n",
       "  ('care', 1): 12,\n",
       "  ('final', 1): 24,\n",
       "  ('fuck', 1): 20,\n",
       "  ('weekend', 1): 61,\n",
       "  ('real', 1): 18,\n",
       "  ('x45', 1): 1,\n",
       "  ('join', 1): 21,\n",
       "  ('hushedcallwithfraydo', 1): 1,\n",
       "  ('gift', 1): 7,\n",
       "  ('yeahhh', 1): 1,\n",
       "  ('hushedpinwithsammi', 1): 2,\n",
       "  ('event', 1): 8,\n",
       "  ('might', 1): 21,\n",
       "  ('luv', 1): 4,\n",
       "  ('realli', 1): 66,\n",
       "  ('appreci', 1): 28,\n",
       "  ('share', 1): 41,\n",
       "  ('wow', 1): 14,\n",
       "  ('tom', 1): 6,\n",
       "  ('3', 1): 27,\n",
       "  ('gym', 1): 3,\n",
       "  ('monday', 1): 7,\n",
       "  ('invit', 1): 15,\n",
       "  ('scope', 1): 5,\n",
       "  ('friend', 1): 43,\n",
       "  ('nude', 1): 1,\n",
       "  ('sleep', 1): 35,\n",
       "  ('birthday', 1): 53,\n",
       "  ('want', 1): 71,\n",
       "  ('t-shirt', 1): 2,\n",
       "  ('cool', 1): 29,\n",
       "  ('haw', 1): 1,\n",
       "  ('phela', 1): 1,\n",
       "  ('mom', 1): 7,\n",
       "  ('obvious', 1): 1,\n",
       "  ('princ', 1): 1,\n",
       "  ('charm', 1): 1,\n",
       "  ('stage', 1): 2,\n",
       "  ('luck', 1): 26,\n",
       "  ('tyler', 1): 1,\n",
       "  ('hipster', 1): 1,\n",
       "  ('glass', 1): 3,\n",
       "  ('marti', 1): 2,\n",
       "  ('glad', 1): 41,\n",
       "  ('done', 1): 40,\n",
       "  ('afternoon', 1): 7,\n",
       "  ('read', 1): 28,\n",
       "  ('kahfi', 1): 1,\n",
       "  ('finish', 1): 15,\n",
       "  ('ohmyg', 1): 1,\n",
       "  ('yaya', 1): 3,\n",
       "  ('dub', 1): 1,\n",
       "  ('stalk', 1): 2,\n",
       "  ('ig', 1): 3,\n",
       "  ('gondooo', 1): 1,\n",
       "  ('moo', 1): 2,\n",
       "  ('tologooo', 1): 1,\n",
       "  ('becom', 1): 8,\n",
       "  ('detail', 1): 8,\n",
       "  ('zzz', 1): 1,\n",
       "  ('xx', 1): 33,\n",
       "  ('physiotherapi', 1): 1,\n",
       "  ('hashtag', 1): 3,\n",
       "  ('💪', 1): 1,\n",
       "  ('monica', 1): 1,\n",
       "  ('miss', 1): 17,\n",
       "  ('sound', 1): 20,\n",
       "  ('morn', 1): 68,\n",
       "  (\"that'\", 1): 49,\n",
       "  ('x43', 1): 1,\n",
       "  ('definit', 1): 20,\n",
       "  ('tri', 1): 34,\n",
       "  ('tonight', 1): 15,\n",
       "  ('took', 1): 7,\n",
       "  ('advic', 1): 6,\n",
       "  ('treviso', 1): 1,\n",
       "  ('concert', 1): 23,\n",
       "  ('citi', 1): 26,\n",
       "  ('countri', 1): 22,\n",
       "  (\"i'll\", 1): 74,\n",
       "  ('start', 1): 56,\n",
       "  ('fine', 1): 7,\n",
       "  ('gorgeou', 1): 9,\n",
       "  ('xo', 1): 2,\n",
       "  ('oven', 1): 2,\n",
       "  ('roast', 1): 1,\n",
       "  ('garlic', 1): 1,\n",
       "  ('oliv', 1): 1,\n",
       "  ('oil', 1): 4,\n",
       "  ('dri', 1): 4,\n",
       "  ('tomato', 1): 1,\n",
       "  ('basil', 1): 1,\n",
       "  ('centuri', 1): 1,\n",
       "  ('tuna', 1): 1,\n",
       "  ('right', 1): 38,\n",
       "  ('atchya', 1): 1,\n",
       "  ('even', 1): 26,\n",
       "  ('almost', 1): 8,\n",
       "  ('chanc', 1): 3,\n",
       "  ('cheer', 1): 18,\n",
       "  ('po', 1): 3,\n",
       "  ('ice', 1): 6,\n",
       "  ('cream', 1): 6,\n",
       "  ('agre', 1): 13,\n",
       "  ('100', 1): 6,\n",
       "  ('heheheh', 1): 2,\n",
       "  ('that', 1): 10,\n",
       "  ('point', 1): 11,\n",
       "  ('stay', 1): 21,\n",
       "  ('home', 1): 20,\n",
       "  ('soon', 1): 38,\n",
       "  ('promis', 1): 4,\n",
       "  ('web', 1): 4,\n",
       "  ('whatsapp', 1): 3,\n",
       "  ('volta', 1): 1,\n",
       "  ('funcionar', 1): 1,\n",
       "  ('com', 1): 2,\n",
       "  ('iphon', 1): 7,\n",
       "  ('jailbroken', 1): 1,\n",
       "  ('later', 1): 12,\n",
       "  ('34', 1): 3,\n",
       "  ('min', 1): 7,\n",
       "  ('leia', 1): 1,\n",
       "  ('appear', 1): 3,\n",
       "  ('hologram', 1): 1,\n",
       "  ('r2d2', 1): 1,\n",
       "  ('w', 1): 16,\n",
       "  ('messag', 1): 9,\n",
       "  ('obi', 1): 1,\n",
       "  ('wan', 1): 1,\n",
       "  ('sit', 1): 7,\n",
       "  ('luke', 1): 4,\n",
       "  ('inter', 1): 1,\n",
       "  ('ucl', 1): 1,\n",
       "  ('arsen', 1): 2,\n",
       "  ('small', 1): 2,\n",
       "  ('team', 1): 24,\n",
       "  ('pass', 1): 10,\n",
       "  ('🚂', 1): 1,\n",
       "  ('dewsburi', 1): 2,\n",
       "  ('railway', 1): 1,\n",
       "  ('station', 1): 4,\n",
       "  ('dew', 1): 1,\n",
       "  ('west', 1): 1,\n",
       "  ('yorkshir', 1): 2,\n",
       "  ('430', 1): 1,\n",
       "  ('smh', 1): 2,\n",
       "  ('9:25', 1): 1,\n",
       "  ('live', 1): 23,\n",
       "  ('strang', 1): 4,\n",
       "  ('imagin', 1): 5,\n",
       "  ('megan', 1): 1,\n",
       "  ('masaantoday', 1): 4,\n",
       "  ('a4', 1): 3,\n",
       "  ('shweta', 1): 1,\n",
       "  ('tripathi', 1): 1,\n",
       "  ('5', 1): 15,\n",
       "  ('20', 1): 5,\n",
       "  ('kurta', 1): 3,\n",
       "  ('half', 1): 6,\n",
       "  ('number', 1): 11,\n",
       "  ('wsalelov', 1): 14,\n",
       "  ('ah', 1): 12,\n",
       "  ('larri', 1): 3,\n",
       "  ('anyway', 1): 14,\n",
       "  ('kinda', 1): 12,\n",
       "  ('goood', 1): 1,\n",
       "  ('life', 1): 36,\n",
       "  ('enn', 1): 1,\n",
       "  ('could', 1): 25,\n",
       "  ('warmup', 1): 1,\n",
       "  ('15th', 1): 2,\n",
       "  ('bath', 1): 6,\n",
       "  ('dum', 1): 2,\n",
       "  ('andar', 1): 1,\n",
       "  ('ram', 1): 1,\n",
       "  ('sampath', 1): 1,\n",
       "  ('sona', 1): 1,\n",
       "  ('mohapatra', 1): 1,\n",
       "  ('samantha', 1): 1,\n",
       "  ('edward', 1): 1,\n",
       "  ('mein', 1): 1,\n",
       "  ('tulan', 1): 1,\n",
       "  ('razi', 1): 2,\n",
       "  ('wah', 1): 2,\n",
       "  ('josh', 1): 1,\n",
       "  ('alway', 1): 48,\n",
       "  ('smile', 1): 47,\n",
       "  ('pictur', 1): 7,\n",
       "  ('16.20', 1): 1,\n",
       "  ('giveitup', 1): 1,\n",
       "  ('given', 1): 3,\n",
       "  ('ga', 1): 3,\n",
       "  ('subsidi', 1): 1,\n",
       "  ('initi', 1): 2,\n",
       "  ('propos', 1): 3,\n",
       "  ('delight', 1): 4,\n",
       "  ('yesterday', 1): 4,\n",
       "  ('x42', 1): 1,\n",
       "  ('lmaoo', 1): 2,\n",
       "  ('song', 1): 16,\n",
       "  ('ever', 1): 19,\n",
       "  ('shall', 1): 5,\n",
       "  ('littl', 1): 29,\n",
       "  ('throwback', 1): 3,\n",
       "  ('outli', 1): 1,\n",
       "  ('island', 1): 2,\n",
       "  ('cheung', 1): 1,\n",
       "  ('chau', 1): 1,\n",
       "  ('mui', 1): 1,\n",
       "  ('wo', 1): 1,\n",
       "  ('total', 1): 6,\n",
       "  ('differ', 1): 10,\n",
       "  ('kfckitchentour', 1): 2,\n",
       "  ('kitchen', 1): 3,\n",
       "  ('clean', 1): 1,\n",
       "  (\"i'm\", 1): 140,\n",
       "  ('cusp', 1): 1,\n",
       "  ('test', 1): 7,\n",
       "  ('water', 1): 7,\n",
       "  ('reward', 1): 1,\n",
       "  ('arummzz', 1): 2,\n",
       "  (\"let'\", 1): 20,\n",
       "  ('drive', 1): 9,\n",
       "  ('travel', 1): 19,\n",
       "  ('yogyakarta', 1): 3,\n",
       "  ('jeep', 1): 3,\n",
       "  ('indonesia', 1): 3,\n",
       "  ('instamood', 1): 3,\n",
       "  ('wanna', 1): 23,\n",
       "  ('skype', 1): 3,\n",
       "  ('may', 1): 16,\n",
       "  ('nice', 1): 71,\n",
       "  ('friendli', 1): 1,\n",
       "  ('pretend', 1): 2,\n",
       "  ('film', 1): 8,\n",
       "  ('congratul', 1): 9,\n",
       "  ('winner', 1): 3,\n",
       "  ('cheesydelight', 1): 1,\n",
       "  ('contest', 1): 5,\n",
       "  ('address', 1): 8,\n",
       "  ('guy', 1): 48,\n",
       "  ('market', 1): 5,\n",
       "  ('24/7', 1): 1,\n",
       "  ('regret', 1): 4,\n",
       "  ('14', 1): 1,\n",
       "  ('hour', 1): 24,\n",
       "  ('leav', 1): 12,\n",
       "  ('without', 1): 9,\n",
       "  ('delay', 1): 1,\n",
       "  ('actual', 1): 13,\n",
       "  ('easi', 1): 7,\n",
       "  ('guess', 1): 8,\n",
       "  ('train', 1): 7,\n",
       "  ('wd', 1): 1,\n",
       "  ('shift', 1): 4,\n",
       "  ('engin', 1): 1,\n",
       "  ('etc', 1): 2,\n",
       "  ('sunburn', 1): 1,\n",
       "  ('peel', 1): 2,\n",
       "  ('blog', 1): 27,\n",
       "  ('huge', 1): 9,\n",
       "  ('warm', 1): 4,\n",
       "  ('☆', 1): 3,\n",
       "  ('complet', 1): 10,\n",
       "  ('triangl', 1): 2,\n",
       "  ('northern', 1): 1,\n",
       "  ('ireland', 1): 2,\n",
       "  ('sight', 1): 1,\n",
       "  ('smthng', 1): 2,\n",
       "  ('fr', 1): 3,\n",
       "  ('hug', 1): 11,\n",
       "  ('xoxo', 1): 3,\n",
       "  ('uu', 1): 1,\n",
       "  ('jaann', 1): 1,\n",
       "  ('topnewfollow', 1): 2,\n",
       "  ('connect', 1): 13,\n",
       "  ('wonder', 1): 26,\n",
       "  ('made', 1): 38,\n",
       "  ('fluffi', 1): 1,\n",
       "  ('insid', 1): 7,\n",
       "  ('pirouett', 1): 1,\n",
       "  ('moos', 1): 1,\n",
       "  ('trip', 1): 12,\n",
       "  ('philli', 1): 1,\n",
       "  ('decemb', 1): 2,\n",
       "  (\"i'd\", 1): 13,\n",
       "  ('dude', 1): 6,\n",
       "  ('x41', 1): 1,\n",
       "  ('question', 1): 15,\n",
       "  ('flaw', 1): 1,\n",
       "  ('pain', 1): 8,\n",
       "  ('negat', 1): 1,\n",
       "  ('strength', 1): 2,\n",
       "  ('went', 1): 10,\n",
       "  ('solo', 1): 4,\n",
       "  ('move', 1): 9,\n",
       "  ('fav', 1): 11,\n",
       "  ('nirvana', 1): 1,\n",
       "  ('smell', 1): 2,\n",
       "  ('teen', 1): 3,\n",
       "  ('spirit', 1): 1,\n",
       "  ('rip', 1): 3,\n",
       "  ('ami', 1): 4,\n",
       "  ('winehous', 1): 1,\n",
       "  ('coupl', 1): 5,\n",
       "  ('tomhiddleston', 1): 1,\n",
       "  ('elizabetholsen', 1): 1,\n",
       "  ('yaytheylookgreat', 1): 1,\n",
       "  ('goodnight', 1): 18,\n",
       "  ('vid', 1): 8,\n",
       "  ('wake', 1): 10,\n",
       "  ('gonna', 1): 16,\n",
       "  ('shoot', 1): 5,\n",
       "  ('itti', 1): 2,\n",
       "  ('bitti', 1): 2,\n",
       "  ('teeni', 1): 2,\n",
       "  ('bikini', 1): 3,\n",
       "  ('much', 1): 73,\n",
       "  ('4th', 1): 4,\n",
       "  ('togeth', 1): 6,\n",
       "  ('end', 1): 13,\n",
       "  ('xfile', 1): 1,\n",
       "  ('content', 1): 3,\n",
       "  ('rain', 1): 18,\n",
       "  ('fabul', 1): 4,\n",
       "  ('fantast', 1): 9,\n",
       "  ('♡', 1): 12,\n",
       "  ('jb', 1): 1,\n",
       "  ('forev', 1): 5,\n",
       "  ('belieb', 1): 3,\n",
       "  ('nighti', 1): 1,\n",
       "  ('bug', 1): 2,\n",
       "  ('bite', 1): 1,\n",
       "  ('bracelet', 1): 2,\n",
       "  ('idea', 1): 24,\n",
       "  ('foundri', 1): 1,\n",
       "  ('game', 1): 23,\n",
       "  ('sens', 1): 6,\n",
       "  ('pic', 1): 21,\n",
       "  ('ef', 1): 1,\n",
       "  ('phone', 1): 16,\n",
       "  ('woot', 1): 2,\n",
       "  ('derek', 1): 1,\n",
       "  ('use', 1): 32,\n",
       "  ('parkshar', 1): 1,\n",
       "  ('gloucestershir', 1): 1,\n",
       "  ('aaaahhh', 1): 1,\n",
       "  ('man', 1): 16,\n",
       "  ('traffic', 1): 2,\n",
       "  ('stress', 1): 4,\n",
       "  ('reliev', 1): 1,\n",
       "  (\"how'r\", 1): 1,\n",
       "  ('arbeloa', 1): 1,\n",
       "  ('turn', 1): 14,\n",
       "  ('17', 1): 2,\n",
       "  ('omg', 1): 13,\n",
       "  ('say', 1): 43,\n",
       "  ('europ', 1): 1,\n",
       "  ('rise', 1): 2,\n",
       "  ('find', 1): 22,\n",
       "  ('hard', 1): 9,\n",
       "  ('believ', 1): 7,\n",
       "  ('uncount', 1): 1,\n",
       "  ('coz', 1): 2,\n",
       "  ('unlimit', 1): 1,\n",
       "  ('cours', 1): 11,\n",
       "  ('teamposit', 1): 1,\n",
       "  ('aldub', 1): 2,\n",
       "  ('☕', 1): 3,\n",
       "  ('rita', 1): 2,\n",
       "  ('info', 1): 11,\n",
       "  (\"we'd\", 1): 4,\n",
       "  ('way', 1): 34,\n",
       "  ('boy', 1): 13,\n",
       "  ('x40', 1): 1,\n",
       "  ('true', 1): 19,\n",
       "  ('sethi', 1): 2,\n",
       "  ('high', 1): 6,\n",
       "  ('exe', 1): 1,\n",
       "  ('skeem', 1): 1,\n",
       "  ('saam', 1): 1,\n",
       "  ('peopl', 1): 43,\n",
       "  ('polit', 1): 2,\n",
       "  ('izzat', 1): 1,\n",
       "  ('wese', 1): 1,\n",
       "  ('trust', 1): 7,\n",
       "  ('khawateen', 1): 1,\n",
       "  ('k', 1): 8,\n",
       "  ('sath', 1): 2,\n",
       "  ('mana', 1): 1,\n",
       "  ('kar', 1): 1,\n",
       "  ('deya', 1): 1,\n",
       "  ('sort', 1): 7,\n",
       "  ('smart', 1): 5,\n",
       "  ('hair', 1): 7,\n",
       "  ('tbh', 1): 5,\n",
       "  ('jacob', 1): 2,\n",
       "  ('g', 1): 8,\n",
       "  ('upgrad', 1): 2,\n",
       "  ('tee', 1): 3,\n",
       "  ('famili', 1): 14,\n",
       "  ('person', 1): 14,\n",
       "  ('two', 1): 15,\n",
       "  ('convers', 1): 6,\n",
       "  ('onlin', 1): 4,\n",
       "  ('mclaren', 1): 1,\n",
       "  ('fridayfeel', 1): 5,\n",
       "  ('tgif', 1): 8,\n",
       "  ('squar', 1): 1,\n",
       "  ('enix', 1): 1,\n",
       "  ('bissmillah', 1): 1,\n",
       "  ('ya', 1): 19,\n",
       "  ('allah', 1): 3,\n",
       "  (\"we'r\", 1): 26,\n",
       "  ('socent', 1): 1,\n",
       "  ('startup', 1): 2,\n",
       "  ('drop', 1): 9,\n",
       "  ('your', 1): 3,\n",
       "  ('arnd', 1): 1,\n",
       "  ('town', 1): 3,\n",
       "  ('basic', 1): 4,\n",
       "  ('piss', 1): 2,\n",
       "  ('cup', 1): 4,\n",
       "  ('also', 1): 29,\n",
       "  ('terribl', 1): 2,\n",
       "  ('complic', 1): 1,\n",
       "  ('discuss', 1): 2,\n",
       "  ('snapchat', 1): 31,\n",
       "  ('lynettelow', 1): 1,\n",
       "  ('kikmenow', 1): 2,\n",
       "  ('snapm', 1): 1,\n",
       "  ('hot', 1): 20,\n",
       "  ('amazon', 1): 1,\n",
       "  ('kikmeguy', 1): 2,\n",
       "  ('defin', 1): 2,\n",
       "  ('grow', 1): 6,\n",
       "  ('sport', 1): 4,\n",
       "  ('rt', 1): 9,\n",
       "  ('rakyat', 1): 1,\n",
       "  ('write', 1): 11,\n",
       "  ('sinc', 1): 11,\n",
       "  ('mention', 1): 18,\n",
       "  ('fli', 1): 5,\n",
       "  ('fish', 1): 4,\n",
       "  ('promot', 1): 3,\n",
       "  ('post', 1): 16,\n",
       "  ('cyber', 1): 1,\n",
       "  ('ourdaughtersourprid', 1): 3,\n",
       "  ('mypapamyprid', 1): 2,\n",
       "  ('papa', 1): 1,\n",
       "  ('coach', 1): 2,\n",
       "  ('posit', 1): 3,\n",
       "  ('kha', 1): 1,\n",
       "  ('atleast', 1): 2,\n",
       "  ('x39', 1): 1,\n",
       "  ('mango', 1): 1,\n",
       "  (\"lassi'\", 1): 1,\n",
       "  (\"monty'\", 1): 1,\n",
       "  ('marvel', 1): 2,\n",
       "  ('though', 1): 16,\n",
       "  ('suspect', 1): 3,\n",
       "  ('meant', 1): 2,\n",
       "  ('24', 1): 3,\n",
       "  ('hr', 1): 2,\n",
       "  ('touch', 1): 7,\n",
       "  ('kepler', 1): 3,\n",
       "  ('452b', 1): 4,\n",
       "  ('chalna', 1): 1,\n",
       "  ('hai', 1): 7,\n",
       "  ('thankyou', 1): 12,\n",
       "  ('hazel', 1): 1,\n",
       "  ('food', 1): 10,\n",
       "  ('brooklyn', 1): 1,\n",
       "  ('pta', 1): 2,\n",
       "  ('awak', 1): 8,\n",
       "  ('okayi', 1): 2,\n",
       "  ('awww', 1): 12,\n",
       "  ('ha', 1): 18,\n",
       "  ('doc', 1): 1,\n",
       "  ('splendid', 1): 1,\n",
       "  ('spam', 1): 1,\n",
       "  ('folder', 1): 1,\n",
       "  ('amount', 1): 1,\n",
       "  ('nigeria', 1): 1,\n",
       "  ('claim', 1): 1,\n",
       "  ('rted', 1): 1,\n",
       "  ('leg', 1): 3,\n",
       "  ('hurt', 1): 4,\n",
       "  ('bad', 1): 14,\n",
       "  ('mine', 1): 11,\n",
       "  ('saturday', 1): 5,\n",
       "  ('thaaank', 1): 1,\n",
       "  ('puhon', 1): 1,\n",
       "  ('happinesss', 1): 1,\n",
       "  ('tnc', 1): 1,\n",
       "  ('prior', 1): 1,\n",
       "  ('notif', 1): 2,\n",
       "  ('probabl', 1): 8,\n",
       "  ('funni', 1): 16,\n",
       "  ('2:22', 1): 1,\n",
       "  ('fat', 1): 1,\n",
       "  ('co', 1): 1,\n",
       "  ('ate', 1): 4,\n",
       "  ('yuna', 1): 2,\n",
       "  ('tamesid', 1): 1,\n",
       "  ('´', 1): 3,\n",
       "  ('googl', 1): 5,\n",
       "  ('account', 1): 17,\n",
       "  ('scouser', 1): 1,\n",
       "  ('everyth', 1): 10,\n",
       "  ('zoe', 1): 1,\n",
       "  ('mate', 1): 5,\n",
       "  ('liter', 1): 6,\n",
       "  (\"they'r\", 1): 10,\n",
       "  ('samee', 1): 1,\n",
       "  ('edgar', 1): 1,\n",
       "  ('updat', 1): 12,\n",
       "  ('log', 1): 3,\n",
       "  ('bring', 1): 14,\n",
       "  ('abe', 1): 1,\n",
       "  ('meet', 1): 26,\n",
       "  ('x38', 1): 1,\n",
       "  ('sigh', 1): 3,\n",
       "  ('dreamili', 1): 1,\n",
       "  ('pout', 1): 1,\n",
       "  ('eye', 1): 12,\n",
       "  ('quacketyquack', 1): 6,\n",
       "  ('happen', 1): 13,\n",
       "  ('phil', 1): 1,\n",
       "  ('em', 1): 2,\n",
       "  ('del', 1): 1,\n",
       "  ('rodder', 1): 1,\n",
       "  ('els', 1): 8,\n",
       "  ('play', 1): 37,\n",
       "  ('newest', 1): 1,\n",
       "  ('gamejam', 1): 1,\n",
       "  ('irish', 1): 2,\n",
       "  ('literatur', 1): 2,\n",
       "  ('inaccess', 1): 2,\n",
       "  (\"kareena'\", 1): 2,\n",
       "  ('fan', 1): 21,\n",
       "  ('brain', 1): 10,\n",
       "  ('dot', 1): 8,\n",
       "  ('braindot', 1): 8,\n",
       "  ('fair', 1): 4,\n",
       "  ('rush', 1): 1,\n",
       "  ('either', 1): 10,\n",
       "  ('brandi', 1): 1,\n",
       "  ('18', 1): 5,\n",
       "  ('carniv', 1): 1,\n",
       "  ('men', 1): 8,\n",
       "  ('put', 1): 11,\n",
       "  ('mask', 1): 2,\n",
       "  ('xavier', 1): 1,\n",
       "  ('forneret', 1): 1,\n",
       "  ('jennif', 1): 1,\n",
       "  ('site', 1): 7,\n",
       "  ('free', 1): 32,\n",
       "  ('50.000', 1): 3,\n",
       "  ('8', 1): 11,\n",
       "  ('ball', 1): 7,\n",
       "  ('pool', 1): 5,\n",
       "  ('coin', 1): 5,\n",
       "  ('edit', 1): 6,\n",
       "  ('trish', 1): 1,\n",
       "  ('♥', 1): 13,\n",
       "  ('grate', 1): 5,\n",
       "  ('three', 1): 8,\n",
       "  ('comment', 1): 8,\n",
       "  ('wakeup', 1): 1,\n",
       "  ('besid', 1): 2,\n",
       "  ('dirti', 1): 2,\n",
       "  ('sex', 1): 4,\n",
       "  ('lmaooo', 1): 1,\n",
       "  ('😤', 1): 2,\n",
       "  ('loui', 1): 4,\n",
       "  (\"he'\", 1): 11,\n",
       "  ('throw', 1): 3,\n",
       "  ('caus', 1): 11,\n",
       "  ('inspir', 1): 6,\n",
       "  ('ff', 1): 40,\n",
       "  ('twoof', 1): 3,\n",
       "  ('gr8', 1): 1,\n",
       "  ('wkend', 1): 3,\n",
       "  ('kind', 1): 22,\n",
       "  ('exhaust', 1): 2,\n",
       "  ('word', 1): 17,\n",
       "  ('cheltenham', 1): 1,\n",
       "  ('area', 1): 4,\n",
       "  ('9', 1): 3,\n",
       "  ('kale', 1): 1,\n",
       "  ('crisp', 1): 1,\n",
       "  ('ruin', 1): 5,\n",
       "  ('x37', 1): 1,\n",
       "  ('open', 1): 12,\n",
       "  ('worldwid', 1): 2,\n",
       "  ('outta', 1): 1,\n",
       "  ('sfvbeta', 1): 1,\n",
       "  ('vantast', 1): 1,\n",
       "  ('xcylin', 1): 1,\n",
       "  ('bundl', 1): 1,\n",
       "  ('show', 1): 20,\n",
       "  ('internet', 1): 2,\n",
       "  ('price', 1): 3,\n",
       "  ('realisticli', 1): 1,\n",
       "  ('pay', 1): 8,\n",
       "  ('net', 1): 1,\n",
       "  ('educ', 1): 1,\n",
       "  ('power', 1): 6,\n",
       "  ('weapon', 1): 1,\n",
       "  ('nelson', 1): 1,\n",
       "  ('mandela', 1): 1,\n",
       "  ('recent', 1): 8,\n",
       "  ('j', 1): 2,\n",
       "  ('chenab', 1): 1,\n",
       "  ('flow', 1): 5,\n",
       "  ('pakistan', 1): 1,\n",
       "  ('incredibleindia', 1): 1,\n",
       "  ('teenchoic', 1): 7,\n",
       "  ('choiceinternationalartist', 1): 7,\n",
       "  ('superjunior', 1): 7,\n",
       "  ('caught', 1): 4,\n",
       "  ('first', 1): 41,\n",
       "  ('salmon', 1): 1,\n",
       "  ('super-blend', 1): 1,\n",
       "  ('project', 1): 6,\n",
       "  ('youth.org.uk', 1): 1,\n",
       "  ('awesom', 1): 35,\n",
       "  ('stream', 1): 12,\n",
       "  ('artist', 1): 2,\n",
       "  ('alma', 1): 1,\n",
       "  ('mater', 1): 1,\n",
       "  ('highschoolday', 1): 1,\n",
       "  ('clientvisit', 1): 1,\n",
       "  ('faith', 1): 3,\n",
       "  ('christian', 1): 1,\n",
       "  ('school', 1): 9,\n",
       "  ('lizaminnelli', 1): 1,\n",
       "  ('upcom', 1): 2,\n",
       "  ('uk', 1): 4,\n",
       "  ('😄', 1): 3,\n",
       "  ('singl', 1): 4,\n",
       "  ('hill', 1): 4,\n",
       "  ('everi', 1): 23,\n",
       "  ('beat', 1): 7,\n",
       "  ('wrong', 1): 9,\n",
       "  ('readi', 1): 22,\n",
       "  ('natur', 1): 1,\n",
       "  ...})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FREQUENCY DICT \n",
    "\n",
    "frequency_token = {}\n",
    "\n",
    "# ITERATE EVERY LABEL AND TWEETS\n",
    "for label , tweets in zip(train_label, tweet_stemmed):\n",
    "    for word in tweets:\n",
    "        pair = (word, int(label))\n",
    "\n",
    "        if pair in frequency_token:\n",
    "            frequency_token[pair] += 1\n",
    "        else:\n",
    "            frequency_token[pair] = 1\n",
    "\n",
    "# DISPLAY RESULT \n",
    "len(frequency_token) , frequency_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before we train the model , create Extract Features First <br><br>\n",
    "Extract Features to faster Training and use them for training instead tweet dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD EXTRACT FEATURES FUNCTION\n",
    "\n",
    "def Extract_Feature(tweet, frequency_token):\n",
    "\n",
    "    # CREATE ARRAY WITH 3 INDEX VALUE 0\n",
    "    index = np.zeros(shape=3)\n",
    "\n",
    "    '''\n",
    "    index[0] is bias with initial value 1\n",
    "    index[1] is sum of frequency every word/token in positive label\n",
    "    index[2] is sum of frequency every word/token in negative label\n",
    "    '''\n",
    "\n",
    "    index[0] = 1  # DECLARE BIAS\n",
    "    \n",
    "    for word in tweet:\n",
    "        index[1] += frequency_token.get((word, 1) , 0)     # TOTAL FREQUENCY WORD IN POSITIVE LABELS\n",
    "        index[2] += frequency_token.get((word, 0) , 0)     # TOTAL FREQUENCY WORD IN NEGATIVE LABELS\n",
    "\n",
    "\n",
    "    # ADD BATCH DIMENSION , SO IT CAN BE TRAIN\n",
    "    index = index[None, :]\n",
    "\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 3.133e+03, 6.100e+01],\n",
       "       [1.000e+00, 3.705e+03, 4.440e+02],\n",
       "       [1.000e+00, 3.119e+03, 1.160e+02],\n",
       "       ...,\n",
       "       [1.000e+00, 1.440e+02, 7.930e+02],\n",
       "       [1.000e+00, 2.070e+02, 3.902e+03],\n",
       "       [1.000e+00, 1.910e+02, 3.986e+03]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATE FEATURE EXTRACTION WITH shape=(8000,3)\n",
    "feature_extraction = np.zeros(shape= (len(train_data), 3))\n",
    "\n",
    "# EXTRACT FEATURE FOR EACH TWEET IN TRAIN DATA\n",
    "for i in range(len(train_data)):\n",
    "    feature_extraction[i,:] = Extract_Feature(tweet_stemmed[i], frequency_token)\n",
    "\n",
    "feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Logistic Regression Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make Sigmoid Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<strong>Sigmoid Formula :      σ(x)= 1 / (1+e^−x)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SIGMOID FUNCTION\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next create Gradient Descent formula to update Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Gradient Descent Formula :  θ --> θ − α . ∇θJ(θ)\n",
    "∇ θJ(θ)= 1/m * (X^T * (h−y))\n",
    "where : \n",
    "       ∇θJ(θ)  = First Derivative from that Cost Function\n",
    "       α       = Learning Rate\n",
    "       θ --> θ = Update Parameter (B0 , B1 , .... Bn)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE GRADIENT DESCENT\n",
    "\n",
    "def gradient_descent(x, y, theta, learning_rate, num_iterations):\n",
    "\n",
    "    N = len(y)  # NUMBER OF SAMPLE DATA\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # COMPUTE Z VALUE (logit value)\n",
    "        z = np.dot(x , theta)\n",
    "\n",
    "        # TRANSFORM Z INTO VALUE BETWEEN 0 - 1\n",
    "        y_pred = sigmoid(z)\n",
    "\n",
    "        # DISPLAY LOSS USING BINARY CROSS ENTROPY\n",
    "        J = (-1/N) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "        # UPDATE PARAMETER \n",
    "        loss     = y_pred - y\n",
    "        gradient = 1 / N * np.dot(x.T, loss )\n",
    "        theta    = theta - learning_rate * gradient\n",
    "\n",
    "    J = float(J)\n",
    "\n",
    "    return J , theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after we create formula for Logistic Regression , now we fit them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Function :  0.09040495433090542\n",
      "\n",
      "Parameter : \n",
      " [[ 4.07804451e-07]\n",
      " [ 1.47199549e-03]\n",
      " [-1.23819836e-03]]\n"
     ]
    }
   ],
   "source": [
    "# TRAIN THE MODEL\n",
    "\n",
    "cost , theta = gradient_descent(x = feature_extraction, y = train_label, theta= np.zeros((3,1)), learning_rate= 1e-8, num_iterations= 1500)\n",
    "\n",
    "print('Cost Function : ', cost)\n",
    "print(f'\\nParameter : \\n', theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT A TWEET \n",
    "\n",
    "def predict_tweet(tweet, theta, frequency_token):\n",
    "\n",
    "    # DO PRE-PROCESS TEXT\n",
    "    clean_tweet   = TextCleaning(tweet)\n",
    "    tweet_token   = Tokenization(clean_tweet)\n",
    "    cleaned_token = remove_StopWords(tweet_token)\n",
    "    stemmed_token = stemming(cleaned_token)\n",
    "\n",
    "    # TRANSFORM INTO EXTRACTED FEATURE\n",
    "    tweet_extract = Extract_Feature(stemmed_token, frequency_token)\n",
    "\n",
    "    # COMPUTE LOGIT VALUE\n",
    "    z = np.dot(tweet_extract , theta)\n",
    "\n",
    "    y_pred = sigmoid(z)\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Tweets : \n",
      " Tweet 1 : Found some goodwill gold today :-). --> [[0.70096132]]\n",
      " Tweet 2 : Stats for the day have arrived. 2 new followers and NO unfollowers :) via http://t.co/6LyskBfqFG. --> [[0.99452528]]\n",
      " Tweet 3 : @MSOpinion glad you liked :) @sureshbabu_ --> [[0.98859708]]\n",
      " Tweet 4 : @EthanGamerTV @DiamondMinecart Yep it is :) --> [[0.98736718]]\n",
      " Tweet 5 : @envydanneh need to put me in that rotation lol :) --> [[0.98762845]]\n",
      "\n",
      "Negative Tweets : \n",
      " Tweet 1 : @Charliescoco @reeceftcharliie @SimonCowell too late :( --> [[0.01034391]]\n",
      " Tweet 2 : @idgitadhg :-( i'm sorry --> [[0.3255905]]\n",
      " Tweet 3 : AP won't be the same anymore :-( --> [[0.37852449]]\n",
      " Tweet 4 : sooooo tired but I can't sleep :((( --> [[0.00843077]]\n",
      " Tweet 5 : I feel so sick :(((((( --> [[0.00900718]]\n"
     ]
    }
   ],
   "source": [
    "# LETS PREDICT SOME TWEET FROM POSITIVE AND NEGATIVE TWEETS\n",
    "\n",
    "print('Positive Tweets : ')\n",
    "for i in range(0,5):\n",
    "    random_index = random.randint(0, len(all_positive_tweets) - 1)\n",
    "    print(f' Tweet {i+1} : {all_positive_tweets[random_index]} --> {predict_tweet(all_positive_tweets[random_index], theta , frequency_token)}')\n",
    "\n",
    "print()\n",
    "\n",
    "print('Negative Tweets : ')\n",
    "for i in range(0,5):\n",
    "    random_index = random.randint(0, len(all_negative_tweets) - 1)\n",
    "    print(f' Tweet {i+1} : {all_negative_tweets[random_index]} --> {predict_tweet(all_negative_tweets[random_index], theta , frequency_token)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the value exceeds 0.5, the tweet is considered positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we predict the word that didnt exist in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1 : I’m so tired of all the negativity online. Can we please have some peace and kindness? --> [[0.44936738]]\n",
      "Word 2 : The new movie was incredible! I loved every minute of it. Highly recommend it! 🎬😄 --> [[0.61946661]]\n",
      "Word 2 : This taste looks Trash! never come back again :( --> [[0.01212026]]\n",
      "Word 2 : Today was full of joy and laughter. So thankful for the wonderful people in my life. #grateful --> [[0.67591974]]\n"
     ]
    }
   ],
   "source": [
    "# PREDICT ANOTHER WORD\n",
    "\n",
    "word_1 = 'I’m so tired of all the negativity online. Can we please have some peace and kindness?'\n",
    "word_2 = 'The new movie was incredible! I loved every minute of it. Highly recommend it! 🎬😄'\n",
    "word_3 = 'This taste looks Trash! never come back again :('\n",
    "word_4 = 'Today was full of joy and laughter. So thankful for the wonderful people in my life. #grateful'\n",
    "\n",
    "print(f'Word 1 : {word_1} --> {predict_tweet(word_1, theta , frequency_token)}')\n",
    "print(f'Word 2 : {word_2} --> {predict_tweet(word_2, theta , frequency_token)}')\n",
    "print(f'Word 2 : {word_3} --> {predict_tweet(word_3, theta , frequency_token)}')\n",
    "print(f'Word 2 : {word_4} --> {predict_tweet(word_4, theta , frequency_token)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok next lets evaluate Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION MODEL LOGISTIC REGRESSION\n",
    "\n",
    "def evaluation_model(data, label , theta, frequency_token):\n",
    "\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in data:\n",
    "\n",
    "        y_pred = predict_tweet(tweet, theta, frequency_token)\n",
    "        \n",
    "        # CHECK WHETHER TWEET IN THE POSITIVE OR NEGATIVE\n",
    "        if y_pred > 0.5:\n",
    "\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "\n",
    "            y_hat.append(0.0)\n",
    "\n",
    "    accuracy = np.mean(np.array(y_hat) == np.squeeze(label))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Model : 0.994\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluation_model(test_data, test_label, theta, frequency_token)\n",
    "\n",
    "print(f'Accuracy Model : {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The developed model for sentiment analysis using Logistic Regression showed excellent results with an accuracy of 0.994. This shows that the model successfully classified almost all tweets correctly (positive or negative) in the test dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
